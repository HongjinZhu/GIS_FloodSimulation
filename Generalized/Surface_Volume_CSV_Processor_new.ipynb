{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUYRUH-gprjD"
      },
      "source": [
        "# Surface Volume and CSV Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ednkgrprjF"
      },
      "source": [
        "## Imports and Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-21T03:38:50.848956Z",
          "start_time": "2020-10-21T03:38:50.522830Z"
        },
        "id": "TMVLRU78prjG"
      },
      "outputs": [],
      "source": [
        "import os, re, json, yaml, math, sys, glob\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "CFG_PATH = os.path.join(\"..\", \"config.yml\") if os.path.exists(os.path.join(\"..\", \"config.yml\")) else \"config.yml\"\n",
        "cfg = yaml.safe_load(open(CFG_PATH, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "PROJECT   = cfg[\"project_name\"]\n",
        "NUM_DIVS  = int(cfg[\"num_divs\"])\n",
        "DEM_DIR   = cfg[\"dem_folder\"]\n",
        "SV_OUT    = cfg[\"surface_volume_out\"]\n",
        "\n",
        "# Optional overrides (safe defaults provided if omitted)\n",
        "SV_LEVELS = cfg.get(\"sv_heights_m\")                    # e.g., [0.0, 0.5, 1.0, ...]  (meters, absolute water level)\n",
        "SV_MIN    = cfg.get(\"sv_min_height_m\", 0.0)            # used when sv_heights_m absent\n",
        "SV_MAX    = cfg.get(\"sv_max_height_m\", 10.0)\n",
        "SV_STEP   = cfg.get(\"sv_step_height_m\", 0.25)          # 25 cm default resolution\n",
        "CSV_NAME  = cfg.get(\"sv_csv_name_fmt\", f\"{PROJECT}_div{{idx:02d}}.csv\")\n",
        "STRICT_INDEXING = bool(cfg.get(\"sv_strict_indexing\", True))  # if True, enforce exactly NUM_DIVS files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def natural_key(s: str):\n",
        "    \"\"\"Sort like '..._0.tif', '..._1.tif', ..., '..._10.tif' numerically.\"\"\"\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.findall(r'\\d+|\\D+', s)]\n",
        "\n",
        "def extract_first_int(s: str) -> Optional[int]:\n",
        "    m = re.search(r'(\\d+)', Path(s).stem)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def build_levels() -> List[float]:\n",
        "    if isinstance(SV_LEVELS, list) and len(SV_LEVELS) > 0:\n",
        "        return [float(x) for x in SV_LEVELS]\n",
        "    # fall back to range\n",
        "    n = max(1, int(round((SV_MAX - SV_MIN) / SV_STEP)) + 1)\n",
        "    return [SV_MIN + i*SV_STEP for i in range(n)]\n",
        "\n",
        "LEVELS = build_levels()\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def log(msg: str):\n",
        "    print(msg, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "ARCPY_OK = False\n",
        "try:\n",
        "    import arcpy\n",
        "    # 3D tools check\n",
        "    if hasattr(arcpy, \"CheckExtension\") and arcpy.CheckExtension(\"3D\") == \"Available\":\n",
        "        ARCPY_OK = True\n",
        "except Exception:\n",
        "    ARCPY_OK = False\n",
        "\n",
        "if not ARCPY_OK:\n",
        "    import rasterio\n",
        "    import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ungrouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cwd: /mnt/e/CERA/GISSR/GIS_FloodSimulation/Generalized\n",
            "DEM_DIR from config: /mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36\n",
            "DEM_DIR exists: True abs: /mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36\n",
            "Found 74 raster-like files\n",
            "['/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/LM_dem_merged.vrt', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/LM_dem_merged.vrt', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_0.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_0.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_1.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_1.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_10.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_10.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_11.tif', '/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36/new_div_label_11.tif']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os, glob\n",
        "\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"DEM_DIR from config:\", DEM_DIR)\n",
        "p = Path(DEM_DIR)\n",
        "print(\"DEM_DIR exists:\", p.exists(), \"abs:\", p.resolve())\n",
        "\n",
        "# Show a few files in the folder (any case)\n",
        "cands = []\n",
        "for pat in [\"*.tif\",\"*.TIF\",\"*.tiff\",\"*.TIFF\",\"*.img\",\"*.IMG\",\"*.vrt\",\"*.VRT\",\"**/*.tif\",\"**/*.TIF\",\"**/*.vrt\",\"**/*.VRT\"]:\n",
        "    cands += glob.glob(str(p / pat), recursive=True)\n",
        "print(\"Found\", len(cands), \"raster-like files\")\n",
        "print(sorted(cands)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_dem_files(folder: str) -> List[str]:\n",
        "    # Common raster extensions\n",
        "    # pats = [\"*.tif\", \"*.tiff\", \"*.img\", \"*.vrt\"]\n",
        "    pats = [\"*.tif\"]\n",
        "    paths = []\n",
        "    for p in pats:\n",
        "        paths.extend(glob.glob(str(Path(folder) / p)))\n",
        "    # Prefer numeric order by index in filename if present\n",
        "    with_idx = [(p, extract_first_int(p)) for p in paths]\n",
        "    if any(idx is not None for _, idx in with_idx):\n",
        "        with_idx = [(p, (idx if idx is not None else 10**9)) for p, idx in with_idx]\n",
        "        with_idx.sort(key=lambda t: t[1])\n",
        "        return [p for p, _ in with_idx]\n",
        "    # else natural alphabetical\n",
        "    return sorted(paths, key=natural_key)\n",
        "\n",
        "DEM_PATHS = list_dem_files(DEM_DIR)\n",
        "\n",
        "if STRICT_INDEXING and len(DEM_PATHS) != NUM_DIVS:\n",
        "    raise RuntimeError(f\"[Config mismatch] Found {len(DEM_PATHS)} DEM(s) in '{DEM_DIR}', but num_divs={NUM_DIVS}.\")\n",
        "\n",
        "if len(DEM_PATHS) == 0:\n",
        "    raise RuntimeError(f\"No DEM rasters found in '{DEM_DIR}'. Check folder and file extensions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0..3 at 0.25m\n",
        "levels_small = np.round(np.arange(0.0, 3.0 + 0.25, 0.25), 2)      # 13 values\n",
        "# 0..10 at 0.5m (we'll keep 3.5..7 from this)\n",
        "levels_large = np.round(np.arange(0.0, 10.0 + 0.5, 0.5), 2)       # 21 values\n",
        "# Final combined set expected by flood code: 0..3 (0.25) + 3.5..7 (0.5)\n",
        "levels_comb  = np.concatenate([levels_small,\n",
        "                               np.round(np.arange(3.5, 7.0 + 0.5, 0.5), 2)])  # 21 rows total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def surface_volume_arcgis(dem_path: str, heights_m: List[float]) -> List[Tuple[float, float, float, int]]:\n",
        "    \"\"\"\n",
        "    Returns list of (height_m, area_m2, volume_m3, cell_count_placeholder)\n",
        "    ArcGIS SurfaceVolume_3d outputs area/volume BELOW plane (Z = height), which matches flood volume to that level.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    arcpy.CheckOutExtension(\"3D\")\n",
        "    for h in heights_m:\n",
        "        # SurfaceVolume_3d(in_raster, out_table, method, plane_height, ref_plane, z_factor)\n",
        "        # method: \"BELOW\" for volume below a plane\n",
        "        tbl = arcpy.CreateUniqueName(\"svtbl\", \"in_memory\")\n",
        "        try:\n",
        "            arcpy.ddd.SurfaceVolume(dem_path, tbl, \"BELOW\", h, \"HORIZONTAL_PLANE\", 1.0)\n",
        "            # Table has fields: AREA_2D, AREA_3D, VOLUME\n",
        "            area = 0.0\n",
        "            vol  = 0.0\n",
        "            with arcpy.da.SearchCursor(tbl, [\"AREA_2D\", \"VOLUME\"]) as cur:\n",
        "                for a2d, v in cur:\n",
        "                    area += float(a2d or 0.0)\n",
        "                    vol  += float(v or 0.0)\n",
        "            out.append((float(h), float(area), float(vol), -1))\n",
        "        finally:\n",
        "            try:\n",
        "                arcpy.management.Delete(tbl)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return out\n",
        "\n",
        "def pixel_area_from_transform(transform) -> float:\n",
        "    # Assumes near-orthogonal geotransform\n",
        "    # area = |a * e - b * d| where Affine(a,b,c,d,e,f)\n",
        "    return abs(transform.a * transform.e - transform.b * transform.d)\n",
        "\n",
        "def surface_volume_numpy(dem_path: str, heights_m: List[float]) -> List[Tuple[float, float, float, int]]:\n",
        "    \"\"\"\n",
        "    Compute flood 'surface volume' as volume of water required to fill terrain up to level h (m),\n",
        "    and 2D surface area (planimetric) of wetted region. DEM and heights are in meters.\n",
        "    \"\"\"\n",
        "    with rasterio.open(dem_path) as ds:\n",
        "        if ds.crs is None:\n",
        "            raise RuntimeError(f\"{dem_path} has no CRS. Reproject to a projected CRS in meters.\")\n",
        "        if \"degree\" in (ds.crs.linear_units or \"\").lower():\n",
        "            raise RuntimeError(f\"{dem_path} is in degrees. Reproject to a projected CRS (meters) first.\")\n",
        "        nodata = ds.nodata\n",
        "        T = ds.transform\n",
        "        A = pixel_area_from_transform(T)\n",
        "        elev = ds.read(1, masked=False).astype(\"float64\")\n",
        "\n",
        "    if nodata is not None:\n",
        "        mask = elev == nodata\n",
        "        elev[mask] = np.nan\n",
        "\n",
        "    out = []\n",
        "    finite = np.isfinite(elev)\n",
        "    for h in heights_m:\n",
        "        wet = finite & (elev < h)\n",
        "        cells = int(np.count_nonzero(wet))\n",
        "        if cells == 0:\n",
        "            out.append((float(h), 0.0, 0.0, 0))\n",
        "            continue\n",
        "        # area (m^2) = wet cells * pixel area\n",
        "        area_m2 = cells * A\n",
        "        # volume (m^3) = sum((h - z) over wet) * pixel_area\n",
        "        vol_m3 = float(np.nansum((h - elev[wet])) * A)\n",
        "        out.append((float(h), float(area_m2), float(vol_m3), cells))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_csv(rows: List[Tuple[float,float,float,int]], out_csv: str):\n",
        "    import csv\n",
        "    ensure_dir(Path(out_csv).parent.as_posix())\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"height_m\", \"area_m2\", \"volume_m3\", \"cell_count\"])\n",
        "        for h,a,v,c in rows:\n",
        "            w.writerow([f\"{h:.6f}\", f\"{a:.6f}\", f\"{v:.6f}\", c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Surface Volume → CSV ===\n",
            "Config: PROJECT='LM_div36'  NUM_DIVS=36  DEM_DIR='/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36'  OUT='/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/SurfaceVolume_LM_div36'\n",
            "Levels: 41 from 0.000 to 10.000 m\n",
            "Engine: NumPy+rasterio\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/36] new_div_label_0.tif  ->  LM_div36_div00.csv\n",
            "[2/36] new_div_label_1.tif  ->  LM_div36_div01.csv\n",
            "[3/36] new_div_label_2.tif  ->  LM_div36_div02.csv\n",
            "[4/36] new_div_label_3.tif  ->  LM_div36_div03.csv\n",
            "[5/36] new_div_label_4.tif  ->  LM_div36_div04.csv\n",
            "[6/36] new_div_label_5.tif  ->  LM_div36_div05.csv\n",
            "[7/36] new_div_label_6.tif  ->  LM_div36_div06.csv\n",
            "[8/36] new_div_label_7.tif  ->  LM_div36_div07.csv\n",
            "[9/36] new_div_label_8.tif  ->  LM_div36_div08.csv\n",
            "[10/36] new_div_label_9.tif  ->  LM_div36_div09.csv\n",
            "[11/36] new_div_label_10.tif  ->  LM_div36_div10.csv\n",
            "[12/36] new_div_label_11.tif  ->  LM_div36_div11.csv\n",
            "[13/36] new_div_label_12.tif  ->  LM_div36_div12.csv\n",
            "[14/36] new_div_label_13.tif  ->  LM_div36_div13.csv\n",
            "[15/36] new_div_label_14.tif  ->  LM_div36_div14.csv\n",
            "[16/36] new_div_label_15.tif  ->  LM_div36_div15.csv\n",
            "[17/36] new_div_label_16.tif  ->  LM_div36_div16.csv\n",
            "[18/36] new_div_label_17.tif  ->  LM_div36_div17.csv\n",
            "[19/36] new_div_label_18.tif  ->  LM_div36_div18.csv\n",
            "[20/36] new_div_label_19.tif  ->  LM_div36_div19.csv\n",
            "[21/36] new_div_label_20.tif  ->  LM_div36_div20.csv\n",
            "[22/36] new_div_label_21.tif  ->  LM_div36_div21.csv\n",
            "[23/36] new_div_label_22.tif  ->  LM_div36_div22.csv\n",
            "[24/36] new_div_label_23.tif  ->  LM_div36_div23.csv\n",
            "[25/36] new_div_label_24.tif  ->  LM_div36_div24.csv\n",
            "[26/36] new_div_label_25.tif  ->  LM_div36_div25.csv\n",
            "[27/36] new_div_label_26.tif  ->  LM_div36_div26.csv\n",
            "[28/36] new_div_label_27.tif  ->  LM_div36_div27.csv\n",
            "[29/36] new_div_label_28.tif  ->  LM_div36_div28.csv\n",
            "[30/36] new_div_label_29.tif  ->  LM_div36_div29.csv\n",
            "[31/36] new_div_label_30.tif  ->  LM_div36_div30.csv\n",
            "[32/36] new_div_label_31.tif  ->  LM_div36_div31.csv\n",
            "[33/36] new_div_label_32.tif  ->  LM_div36_div32.csv\n",
            "[34/36] new_div_label_33.tif  ->  LM_div36_div33.csv\n",
            "[35/36] new_div_label_34.tif  ->  LM_div36_div34.csv\n",
            "[36/36] new_div_label_35.tif  ->  LM_div36_div35.csv\n",
            "Done. Wrote 36 CSV(s).\n",
            "Manifest: /mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/SurfaceVolume_LM_div36/LM_div36_sv_manifest.json\n"
          ]
        }
      ],
      "source": [
        "# Main Loop\n",
        "def main():\n",
        "    log(\"=== Surface Volume → CSV ===\")\n",
        "    log(f\"Config: {PROJECT=}  {NUM_DIVS=}  DEM_DIR='{DEM_DIR}'  OUT='{SV_OUT}'\")\n",
        "    log(f\"Levels: {len(LEVELS)} from {LEVELS[0]:.3f} to {LEVELS[-1]:.3f} m\" + (f\"  (custom)\" if cfg.get(\"sv_heights_m\") else \"\"))\n",
        "    log(f\"Engine: {'ArcGIS 3D Analyst' if ARCPY_OK else 'NumPy+rasterio'}\")\n",
        "\n",
        "    manifest = {\n",
        "        \"project\": PROJECT,\n",
        "        \"num_divs\": NUM_DIVS,\n",
        "        \"dem_dir\": DEM_DIR,\n",
        "        \"sv_out\": SV_OUT,\n",
        "        \"levels_m\": LEVELS,\n",
        "        \"engine\": \"arcpy.SurfaceVolume\" if ARCPY_OK else \"numpy_rasterio\",\n",
        "        \"items\": []\n",
        "    }\n",
        "\n",
        "    if STRICT_INDEXING and len(DEM_PATHS) != NUM_DIVS:\n",
        "        log(f\"[WARN] Found {len(DEM_PATHS)} DEM(s); num_divs={NUM_DIVS}. Proceeding anyway due to STRICT_INDEXING={STRICT_INDEXING}.\")\n",
        "\n",
        "    for idx, dem_path in enumerate(DEM_PATHS):\n",
        "        # If STRICT_INDEXING, force idx to be the loop index 0..NUM_DIVS-1\n",
        "        target_idx = idx if STRICT_INDEXING else (extract_first_int(dem_path) or idx)\n",
        "        out_csv = Path(SV_OUT) / CSV_NAME.format(idx=target_idx)\n",
        "\n",
        "        log(f\"[{idx+1}/{len(DEM_PATHS)}] {Path(dem_path).name}  ->  {out_csv.name}\")\n",
        "\n",
        "        try:\n",
        "            if ARCPY_OK:\n",
        "                rows = surface_volume_arcgis(dem_path, LEVELS)\n",
        "            else:\n",
        "                rows = surface_volume_numpy(dem_path, LEVELS)\n",
        "            write_csv(rows, out_csv.as_posix())\n",
        "\n",
        "            manifest[\"items\"].append({\n",
        "                \"index\": int(target_idx),\n",
        "                \"dem\": dem_path,\n",
        "                \"csv\": out_csv.as_posix(),\n",
        "                \"min_height_m\": float(LEVELS[0]),\n",
        "                \"max_height_m\": float(LEVELS[-1]),\n",
        "                \"n_levels\": len(LEVELS)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            log(f\"  !! ERROR on {dem_path}: {e}\")\n",
        "            manifest[\"items\"].append({\n",
        "                \"index\": int(target_idx),\n",
        "                \"dem\": dem_path,\n",
        "                \"csv\": None,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    # Sort manifest by index for convenience\n",
        "    manifest[\"items\"].sort(key=lambda d: d.get(\"index\", 10**9))\n",
        "    ensure_dir(SV_OUT)\n",
        "    man_path = Path(SV_OUT) / f\"{PROJECT}_sv_manifest.json\"\n",
        "    with open(man_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    log(f\"Done. Wrote {len([it for it in manifest['items'] if it.get('csv')])} CSV(s).\")\n",
        "    log(f\"Manifest: {man_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTws0pCprjM"
      },
      "source": [
        "# Grouped Divs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groups.json OK\n"
          ]
        }
      ],
      "source": [
        "import json, yaml, sys\n",
        "cfg = yaml.safe_load(open(\"../config.yml\"))\n",
        "N = cfg[\"num_divs\"]\n",
        "\n",
        "def check_ids(L, name):\n",
        "    bad = [i for i in L if not isinstance(i,int) or i<0 or i>=N]\n",
        "    if bad: raise ValueError(f\"{name}: out-of-range IDs {bad}\")\n",
        "\n",
        "if cfg[\"adjacency\"][\"type\"] == \"groups\":\n",
        "    G = json.load(open(cfg[\"adjacency\"][\"groups_file\"]))\n",
        "    groups = G[\"groups\"] if isinstance(G, dict) else [{\"order\": g} for g in G]\n",
        "    seen = set()\n",
        "    for gi,g in enumerate(groups):\n",
        "        order = g[\"order\"]\n",
        "        check_ids(order, f\"group[{gi}].order\")\n",
        "        if len(set(order))!=len(order):\n",
        "            raise ValueError(f\"group[{gi}] has duplicates\")\n",
        "        overlap = set(order)&seen\n",
        "        if overlap:\n",
        "            raise ValueError(f\"IDs appear in multiple groups: {sorted(overlap)}\")\n",
        "        seen |= set(order)\n",
        "    if isinstance(G, dict) and \"landlocked\" in G:\n",
        "        check_ids(G[\"landlocked\"], \"landlocked\")\n",
        "    print(\"Groups.json OK\")\n",
        "else:\n",
        "    A = json.load(open(cfg[\"adjacency\"][\"graph_file\"]))\n",
        "    edges = A[\"edges\"]\n",
        "    if edges and isinstance(edges[0], list):   # simple form\n",
        "        for u,v in edges:\n",
        "            check_ids([u,v],\"edge\")\n",
        "    else:\n",
        "        for e in edges:\n",
        "            check_ids([e[\"u\"],e[\"v\"]],\"edge\")\n",
        "            if e.get(\"blocked\") not in (None,True,False):\n",
        "                raise ValueError(\"blocked must be true/false if present\")\n",
        "    if \"num_nodes\" in A and A[\"num_nodes\"] != N:\n",
        "        raise ValueError(f\"num_nodes mismatch: {A['num_nodes']} vs {N}\")\n",
        "    if \"landlocked\" in A:\n",
        "        check_ids(A[\"landlocked\"], \"landlocked\")\n",
        "    print(\"Adjacency.json OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Group-aware switches (optional) ----\n",
        "SV_USE_GROUPS       = bool(cfg.get(\"sv_use_groups\", False))\n",
        "SV_ONLY_GROUPS      = bool(cfg.get(\"sv_only_groups\", False))\n",
        "SV_ORDER_BY_GROUPS  = bool(cfg.get(\"sv_order_by_groups\", SV_ONLY_GROUPS))  # default: true if only_groups is true\n",
        "\n",
        "def _normalize_groups(obj):\n",
        "    \"\"\"\n",
        "    Accept both:\n",
        "      - {\"groups\":[{\"name\":\"A\",\"order\":[0,1,2]}, {\"name\":\"B\",\"order\":[10,11]}], \"landlocked\":[...]}\n",
        "      - [[0,1,2],[10,11]]\n",
        "    Return: list of lists (orders), landlocked (list)\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        groups = obj.get(\"groups\", [])\n",
        "        if groups and isinstance(groups[0], dict):\n",
        "            orders = [g[\"order\"] for g in groups]\n",
        "        else:\n",
        "            orders = groups  # if already [[...],[...]]\n",
        "        landlocked = obj.get(\"landlocked\", [])\n",
        "    else:\n",
        "        orders = obj\n",
        "        landlocked = []\n",
        "    # flatten sanity\n",
        "    flat = [i for g in orders for i in g]\n",
        "    if len(set(flat)) != len(flat):\n",
        "        dupes = sorted([i for i in flat if flat.count(i) > 1])\n",
        "        raise ValueError(f\"Groups.json contains duplicate IDs across groups: {dupes}\")\n",
        "    return orders, landlocked\n",
        "\n",
        "def load_groups_from_cfg(cfg, N):\n",
        "    \"\"\"\n",
        "    Read groups if cfg['adjacency']['type'] == 'groups'.\n",
        "    Returns: list_of_orders, landlocked, coastal_ids_flat\n",
        "    \"\"\"\n",
        "    adj = cfg.get(\"adjacency\", {})\n",
        "    if adj.get(\"type\") != \"groups\":\n",
        "        return None, [], []\n",
        "\n",
        "    import json, os\n",
        "    path = adj.get(\"groups_file\")\n",
        "    if not path or not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"adjacency.type='groups' but groups_file not found: {path}\")\n",
        "    obj = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
        "    orders, landlocked = _normalize_groups(obj)\n",
        "\n",
        "    # validate ranges\n",
        "    bad = [i for i in [x for g in orders for x in g] if (not isinstance(i, int)) or i < 0 or i >= N]\n",
        "    if bad:\n",
        "        raise ValueError(f\"Groups.json has out-of-range IDs (0..{N-1}): {sorted(set(bad))}\")\n",
        "\n",
        "    return orders, landlocked, [i for g in orders for i in g]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_first_int(s: str):\n",
        "    import re\n",
        "    m = re.search(r'(\\d+)', Path(s).stem)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def list_dem_files(folder: str):\n",
        "    import glob\n",
        "    root = Path(folder)\n",
        "    pats = [\"*.tif\",\"*.tiff\",\"*.img\",\"*.vrt\",\"**/*.tif\",\"**/*.tiff\",\"**/*.img\",\"**/*.vrt\",\n",
        "            \"*.TIF\",\"*.TIFF\",\"*.IMG\",\"*.VRT\",\"**/*.TIF\",\"**/*.TIFF\",\"**/*.IMG\",\"**/*.VRT\"]\n",
        "    paths = []\n",
        "    for pat in pats:\n",
        "        paths.extend(glob.glob(str(root / pat), recursive=True))\n",
        "    # drop sidecars\n",
        "    paths = [p for p in paths if not p.lower().endswith(\".aux.xml\")]\n",
        "    return sorted(set(paths))\n",
        "\n",
        "DEM_PATHS = list_dem_files(DEM_DIR)\n",
        "\n",
        "if len(DEM_PATHS) == 0:\n",
        "    raise RuntimeError(f\"No DEM rasters found in '{DEM_DIR}'.\")\n",
        "\n",
        "# Map index -> path:\n",
        "# priority 1: use numeric index embedded in filename (e.g., *_12.tif)\n",
        "# priority 2: if STRICT_INDEXING, assign in sorted order (0..NUM_DIVS-1)\n",
        "IDX_TO_DEM = {}\n",
        "by_num = {}\n",
        "for p in DEM_PATHS:\n",
        "    k = extract_first_int(p)\n",
        "    if k is not None:\n",
        "        by_num[k] = p\n",
        "\n",
        "if len(by_num) >= min(NUM_DIVS, len(DEM_PATHS)):\n",
        "    # filenames carry indices; use them\n",
        "    IDX_TO_DEM = by_num\n",
        "else:\n",
        "    # fallback: sequential mapping\n",
        "    if STRICT_INDEXING and len(DEM_PATHS) != NUM_DIVS:\n",
        "        raise RuntimeError(f\"[Config mismatch] Found {len(DEM_PATHS)} DEM(s) in '{DEM_DIR}', but num_divs={NUM_DIVS}.\")\n",
        "    for i, p in enumerate(sorted(DEM_PATHS)):\n",
        "        IDX_TO_DEM[i] = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "GROUP_ORDERS, GROUP_LANDLOCKED, COASTAL_IDS = (None, [], [])\n",
        "if SV_USE_GROUPS:\n",
        "    GROUP_ORDERS, GROUP_LANDLOCKED, COASTAL_IDS = load_groups_from_cfg(cfg, NUM_DIVS)\n",
        "\n",
        "# Decide the processing list of indices\n",
        "if SV_USE_GROUPS and SV_ONLY_GROUPS:\n",
        "    PROCESS_IDX = list(COASTAL_IDS)  # only those in groups\n",
        "else:\n",
        "    # all indices that we have DEMs for\n",
        "    PROCESS_IDX = sorted(IDX_TO_DEM.keys())\n",
        "\n",
        "# Apply ordering preference\n",
        "if SV_USE_GROUPS and SV_ORDER_BY_GROUPS and GROUP_ORDERS:\n",
        "    # concatenate groups in the file order\n",
        "    ordered = []\n",
        "    for order in GROUP_ORDERS:\n",
        "        ordered.extend(order)\n",
        "    # retain only those we intend to process\n",
        "    PROCESS_IDX = [i for i in ordered if i in set(PROCESS_IDX)]\n",
        "# else leave as is (sorted by index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    log(\"=== Surface Volume → CSV ===\")\n",
        "    log(f\"Config: PROJECT={PROJECT}  NUM_DIVS={NUM_DIVS}  DEM_DIR='{DEM_DIR}'  OUT='{SV_OUT}'\")\n",
        "    log(f\"Levels: {len(LEVELS)} from {LEVELS[0]:.3f} to {LEVELS[-1]:.3f} m\")\n",
        "    log(f\"Engine: {'ArcGIS 3D Analyst' if ARCPY_OK else 'NumPy+rasterio'}\")\n",
        "    if SV_USE_GROUPS:\n",
        "        log(f\"Groups: use_groups={SV_USE_GROUPS} only_groups={SV_ONLY_GROUPS} order_by_groups={SV_ORDER_BY_GROUPS}\")\n",
        "        if GROUP_ORDERS:\n",
        "            log(f\"  #groups={len(GROUP_ORDERS)}  coastal_ids={len(COASTAL_IDS)}\")\n",
        "\n",
        "    manifest = {\n",
        "        \"project\": PROJECT,\n",
        "        \"num_divs\": NUM_DIVS,\n",
        "        \"dem_dir\": DEM_DIR,\n",
        "        \"sv_out\": SV_OUT,\n",
        "        \"levels_m\": LEVELS,\n",
        "        \"engine\": \"arcpy.SurfaceVolume\" if ARCPY_OK else \"numpy_rasterio\",\n",
        "        \"use_groups\": SV_USE_GROUPS,\n",
        "        \"only_groups\": SV_ONLY_GROUPS,\n",
        "        \"order_by_groups\": SV_ORDER_BY_GROUPS,\n",
        "        \"groups\": GROUP_ORDERS if GROUP_ORDERS is not None else [],\n",
        "        \"landlocked\": GROUP_LANDLOCKED,\n",
        "        \"items\": []\n",
        "    }\n",
        "\n",
        "    total = len(PROCESS_IDX)\n",
        "    for k, idx in enumerate(PROCESS_IDX, 1):\n",
        "        if idx not in IDX_TO_DEM:\n",
        "            log(f\"[{k}/{total}] idx={idx}: no DEM found; skipping\")\n",
        "            manifest[\"items\"].append({\"index\": int(idx), \"dem\": None, \"csv\": None, \"error\": \"missing DEM\"})\n",
        "            continue\n",
        "\n",
        "        dem_path = IDX_TO_DEM[idx]\n",
        "        out_csv = Path(SV_OUT) / CSV_NAME.format(idx=idx)\n",
        "\n",
        "        log(f\"[{k}/{total}] div {idx:02d}: {Path(dem_path).name} -> {out_csv.name}\")\n",
        "        try:\n",
        "            rows = surface_volume_arcgis(dem_path, LEVELS) if ARCPY_OK else surface_volume_numpy(dem_path, LEVELS)\n",
        "            write_csv(rows, out_csv.as_posix())\n",
        "            manifest[\"items\"].append({\n",
        "                \"index\": int(idx),\n",
        "                \"dem\": dem_path,\n",
        "                \"csv\": out_csv.as_posix(),\n",
        "                \"min_height_m\": float(LEVELS[0]),\n",
        "                \"max_height_m\": float(LEVELS[-1]),\n",
        "                \"n_levels\": len(LEVELS)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            log(f\"  !! ERROR on idx {idx}: {e}\")\n",
        "            manifest[\"items\"].append({\"index\": int(idx), \"dem\": dem_path, \"csv\": None, \"error\": str(e)})\n",
        "\n",
        "    ensure_dir(SV_OUT)\n",
        "    man_path = Path(SV_OUT) / f\"{PROJECT}_sv_manifest.json\"\n",
        "    with open(man_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    ok_cnt = sum(1 for it in manifest[\"items\"] if it.get(\"csv\"))\n",
        "    log(f\"Done. Wrote {ok_cnt}/{total} CSV(s).\")\n",
        "    log(f\"Manifest: {man_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Surface Volume → CSV ===\n",
            "Config: PROJECT=LM_div36  NUM_DIVS=36  DEM_DIR='/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/LM_div36'  OUT='/mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/SurfaceVolume_LM_div36'\n",
            "Levels: 41 from 0.000 to 10.000 m\n",
            "Engine: NumPy+rasterio\n",
            "[1/36] div 00: new_div_label_0.tif -> LM_div36_div00.csv\n",
            "[2/36] div 01: new_div_label_1.tif -> LM_div36_div01.csv\n",
            "[3/36] div 02: new_div_label_2.tif -> LM_div36_div02.csv\n",
            "[4/36] div 03: new_div_label_3.tif -> LM_div36_div03.csv\n",
            "[5/36] div 04: new_div_label_4.tif -> LM_div36_div04.csv\n",
            "[6/36] div 05: new_div_label_5.tif -> LM_div36_div05.csv\n",
            "[7/36] div 06: new_div_label_6.tif -> LM_div36_div06.csv\n",
            "[8/36] div 07: new_div_label_7.tif -> LM_div36_div07.csv\n",
            "[9/36] div 08: new_div_label_8.tif -> LM_div36_div08.csv\n",
            "[10/36] div 09: new_div_label_9.tif -> LM_div36_div09.csv\n",
            "[11/36] div 10: new_div_label_10.tif -> LM_div36_div10.csv\n",
            "[12/36] div 11: new_div_label_11.tif -> LM_div36_div11.csv\n",
            "[13/36] div 12: new_div_label_12.tif -> LM_div36_div12.csv\n",
            "[14/36] div 13: new_div_label_13.tif -> LM_div36_div13.csv\n",
            "[15/36] div 14: new_div_label_14.tif -> LM_div36_div14.csv\n",
            "[16/36] div 15: new_div_label_15.tif -> LM_div36_div15.csv\n",
            "[17/36] div 16: new_div_label_16.tif -> LM_div36_div16.csv\n",
            "[18/36] div 17: new_div_label_17.tif -> LM_div36_div17.csv\n",
            "[19/36] div 18: new_div_label_18.tif -> LM_div36_div18.csv\n",
            "[20/36] div 19: new_div_label_19.tif -> LM_div36_div19.csv\n",
            "[21/36] div 20: new_div_label_20.tif -> LM_div36_div20.csv\n",
            "[22/36] div 21: new_div_label_21.tif -> LM_div36_div21.csv\n",
            "[23/36] div 22: new_div_label_22.tif -> LM_div36_div22.csv\n",
            "[24/36] div 23: new_div_label_23.tif -> LM_div36_div23.csv\n",
            "[25/36] div 24: new_div_label_24.tif -> LM_div36_div24.csv\n",
            "[26/36] div 25: new_div_label_25.tif -> LM_div36_div25.csv\n",
            "[27/36] div 26: new_div_label_26.tif -> LM_div36_div26.csv\n",
            "[28/36] div 27: new_div_label_27.tif -> LM_div36_div27.csv\n",
            "[29/36] div 28: new_div_label_28.tif -> LM_div36_div28.csv\n",
            "[30/36] div 29: new_div_label_29.tif -> LM_div36_div29.csv\n",
            "[31/36] div 30: new_div_label_30.tif -> LM_div36_div30.csv\n",
            "[32/36] div 31: new_div_label_31.tif -> LM_div36_div31.csv\n",
            "[33/36] div 32: new_div_label_32.tif -> LM_div36_div32.csv\n",
            "[34/36] div 33: new_div_label_33.tif -> LM_div36_div33.csv\n",
            "[35/36] div 34: new_div_label_34.tif -> LM_div36_div34.csv\n",
            "[36/36] div 35: new_div_label_35.tif -> LM_div36_div35.csv\n",
            "Done. Wrote 36/36 CSV(s).\n",
            "Manifest: /mnt/e/CERA/GISSR/GIS_FloodSimulation/Data/SurfaceVolume_LM_div36/LM_div36_sv_manifest.json\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "gissr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
